{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   HTHG  HTAG    HS    AS   HST   AST    HF    AF    HC   AC  ...    \\\n",
      "0   0.0   1.0  11.0  14.0   5.0   7.0  15.0  14.0   4.0  6.0  ...     \n",
      "1   0.0   1.0  17.0   8.0   9.0   5.0  12.0   9.0   5.0  4.0  ...     \n",
      "2   0.0   1.0  11.0  20.0   3.0  13.0  16.0  10.0   4.0  7.0  ...     \n",
      "3   1.0   1.0  26.0   7.0  12.0   3.0  13.0  15.0  12.0  4.0  ...     \n",
      "4   0.0   3.0   8.0  15.0   5.0   9.0  11.0  13.0   4.0  9.0  ...     \n",
      "\n",
      "   AwayTeam_Tottenham  AwayTeam_Watford  AwayTeam_West Brom  \\\n",
      "0                 0.0               0.0                 0.0   \n",
      "1                 0.0               0.0                 0.0   \n",
      "2                 0.0               0.0                 0.0   \n",
      "3                 0.0               0.0                 0.0   \n",
      "4                 0.0               0.0                 0.0   \n",
      "\n",
      "   AwayTeam_West Ham  AwayTeam_Wigan  AwayTeam_Wolves  FTR_H  HTR_A  HTR_D  \\\n",
      "0                0.0             1.0              0.0    0.0    1.0    0.0   \n",
      "1                0.0             0.0              0.0    0.0    1.0    0.0   \n",
      "2                0.0             0.0              0.0    0.0    1.0    0.0   \n",
      "3                0.0             0.0              0.0    1.0    0.0    1.0   \n",
      "4                0.0             0.0              0.0    0.0    1.0    0.0   \n",
      "\n",
      "   HTR_H  \n",
      "0    0.0  \n",
      "1    0.0  \n",
      "2    0.0  \n",
      "3    0.0  \n",
      "4    0.0  \n",
      "\n",
      "[5 rows x 90 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('C:\\\\Users\\\\bunny\\\\Desktop\\\\DAAPROJECT\\\\fifadata_csv.csv',delimiter = ',')\n",
    "#df = open('C:\\\\Users\\\\bunny\\\\Desktop\\\\DAAPROJECT\\\\fifadata_csv.csv','rb').read().splitlines()\n",
    "#d = df.read()\n",
    "df = df.drop('Date',1)\n",
    "df = df.drop('Referee',1)\n",
    "#df = df.drop('HTR',1)\n",
    "df = df.drop('FTHG',1)\n",
    "df = df.drop('FTAG',1)\n",
    "\n",
    "#df['FTR'] = pd.to_numeric(df['FTR'])\n",
    "df1=pd.get_dummies(df).astype(np.float)\n",
    "#df['FTR'] = df['FTR'].astype(object)\n",
    "#c = df1.dtypes\n",
    "#print(c)\n",
    "\n",
    "#FTR = {'H': 1,'A': 2,'D': 3}\n",
    "#df1.FTR=[FTR[item] for item in df1.FTR] \n",
    "#df1['FTR'] = pd.to_numeric(df1['FTR'])\n",
    "#c = df1.dtypes\n",
    "#print(c)\n",
    "df1 = df1.drop('FTR_A',1)\n",
    "df1 = df1.drop('FTR_D',1)    \n",
    "train,test = df1.iloc[:3048,],df1.iloc[3048:,]\n",
    "X=train.drop('FTR_H',1)\n",
    "Y=train['FTR_H']\n",
    "X_test=test.drop('FTR_H',1)\n",
    "Y_test=test['FTR_H']\n",
    "#print(Y)\n",
    "print(train.head())\n",
    "#print(test.head())\n",
    "#df.to_csv(r'C:\\\\Users\\\\bunny\\\\Desktop\\\\DAAPROJECT\\File_Name.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Manual Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def sigmoid(scores):\n",
    "    return 1 / (1 + np.exp(-scores))\n",
    "\n",
    "def logistic_regression(features, target, num_steps, learning_rate, add_intercept = False):\n",
    "    if add_intercept:\n",
    "        intercept = np.ones((features.shape[0], 1))\n",
    "        features = np.hstack((intercept, features))\n",
    "        \n",
    "    weights = np.zeros(features.shape[1])\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        scores = np.dot(features, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "\n",
    "        # Update weights with gradient\n",
    "        output_error_signal = target - predictions\n",
    "        gradient = np.dot(features.T, output_error_signal)\n",
    "        weights += learning_rate * gradient\n",
    "        \n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bunny\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.92      0.49      0.64       347\n",
      "        1.0       0.61      0.95      0.74       294\n",
      "\n",
      "avg / total       0.78      0.70      0.69       641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = logistic_regression(X, Y,\n",
    "                     num_steps = 500, learning_rate = 0.1, add_intercept=True)\n",
    "#print(weights)\n",
    "data_with_intercept = np.hstack((np.ones((X_test.shape[0], 1)),\n",
    "                                 X_test))\n",
    "final_scores = np.dot(data_with_intercept, weights)\n",
    "preds = np.round(sigmoid(final_scores))\n",
    "preds\n",
    "#print format((preds == Y_test).sum().astype(float) / len(preds))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression from sklearn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X,Y)\n",
    "predictions = logmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.86      0.82       347\n",
      "        1.0       0.81      0.71      0.75       294\n",
      "\n",
      "avg / total       0.79      0.79      0.79       641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import math\n",
    "#from mlfromscratch.utils import euclidean_distance\n",
    "def euclidean_distance(x1, x2):\n",
    "    \"\"\" Calculates the l2 distance between two vectors \"\"\"\n",
    "    distance = 0\n",
    "    # Squared distance between each coordinate\n",
    "    for i in range(len(x1)):\n",
    "        distance += pow(float((float(x1[i]) - float(x2[i]))), 2)\n",
    "    return math.sqrt(distance)\n",
    "class KNN():\n",
    "    \"\"\" K Nearest Neighbors classifier.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    k: int\n",
    "        The number of closest neighbors that will determine the class of the \n",
    "        sample that we wish to predict.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "\n",
    "    def _vote(self, neighbor_labels):\n",
    "        \"\"\" Return the most common class among the neighbor samples \"\"\"\n",
    "        counts = np.bincount(neighbor_labels.astype('int'))\n",
    "        return counts.argmax()\n",
    "\n",
    "    def predict(self, testing, X_train, y_train):\n",
    "        y_pred = np.empty(testing.shape[0])\n",
    "        # Determine the class of each sample\n",
    "        #print((X_test))\n",
    "        for i, test_sample in enumerate(testing):\n",
    "            #print(i,test_sample)\n",
    "            # Sort the training samples by their distance to the test sample and get the K nearest\n",
    "            idx = np.argsort([euclidean_distance(test_sample, x) for x in X_train])[:self.k]\n",
    "            # Extract the labels of the K nearest neighboring training samples\n",
    "            k_nearest_neighbors = np.array([y_train[i] for i in idx])\n",
    "            # Label sample as the most common class label\n",
    "            y_pred[i] = self._vote(k_nearest_neighbors)\n",
    "        #return 0\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn_model=KNN(k=5)\n",
    "dataX=X.values\n",
    "dataY=Y.values\n",
    "testdataX=X_test.values\n",
    "testdataY=Y_test.values\n",
    "#print(dataX,dataY)\n",
    "clf = KNN(k=5)\n",
    "y_pred = clf.predict(testdataX, dataX, dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.70      0.76      0.73       347\n",
      "        1.0       0.68      0.62      0.65       294\n",
      "\n",
      "avg / total       0.69      0.69      0.69       641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knn from sklearn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.70      0.76      0.73       347\n",
      "        1.0       0.68      0.63      0.65       294\n",
      "\n",
      "avg / total       0.69      0.70      0.69       641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X,Y)\n",
    "y_skpred=knn.predict(X_test)\n",
    "print(classification_report(Y_test,y_skpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "class NaiveBayes():\n",
    "    \"\"\"The Gaussian Naive Bayes classifier. \"\"\"\n",
    "    def fit(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "        self.classes = np.unique(y)\n",
    "        self.parameters = []\n",
    "        # Calculate the mean and variance of each feature for each class\n",
    "        for i, c in enumerate(self.classes):\n",
    "            # Only select the rows where the label equals the given class\n",
    "            X_where_c = X[np.where(y == c)]\n",
    "            self.parameters.append([])\n",
    "            # Add the mean and variance for each feature (column)\n",
    "            for col in X_where_c.T:\n",
    "                parameters = {\"mean\": col.mean(), \"var\": col.var()}\n",
    "                self.parameters[i].append(parameters)\n",
    "\n",
    "    def _calculate_likelihood(self, mean, var, x):\n",
    "        \"\"\" Gaussian likelihood of the data x given mean and var \"\"\"\n",
    "        eps = 1e-4 # Added in denominator to prevent division by zero\n",
    "        coeff = 1.0 / math.sqrt(2.0 * math.pi * var + eps)\n",
    "        exponent = math.exp(-(math.pow(x - mean, 2) / (2 * var + eps)))\n",
    "        return coeff * exponent\n",
    "\n",
    "    def _calculate_prior(self, c):\n",
    "        \"\"\" Calculate the prior of class c\n",
    "        (samples where class == c / total number of samples)\"\"\"\n",
    "        frequency = np.mean(self.y == c)\n",
    "        return frequency\n",
    "\n",
    "    def _classify(self, sample):\n",
    "        \"\"\" Classification using Bayes Rule P(Y|X) = P(X|Y)*P(Y)/P(X),\n",
    "            or Posterior = Likelihood * Prior / Scaling Factor\n",
    "        P(Y|X) - The posterior is the probability that sample x is of class y given the\n",
    "                 feature values of x being distributed according to distribution of y and the prior.\n",
    "        P(X|Y) - Likelihood of data X given class distribution Y.\n",
    "                 Gaussian distribution (given by _calculate_likelihood)\n",
    "        P(Y)   - Prior (given by _calculate_prior)\n",
    "        P(X)   - Scales the posterior to make it a proper probability distribution.\n",
    "                 This term is ignored in this implementation since it doesn't affect\n",
    "                 which class distribution the sample is most likely to belong to.\n",
    "        Classifies the sample as the class that results in the largest P(Y|X) (posterior)\n",
    "        \"\"\"\n",
    "        posteriors = []\n",
    "        # Go through list of classes\n",
    "        for i, c in enumerate(self.classes):\n",
    "            # Initialize posterior as prior\n",
    "            posterior = self._calculate_prior(c)\n",
    "            # Naive assumption (independence):\n",
    "            # P(x1,x2,x3|Y) = P(x1|Y)*P(x2|Y)*P(x3|Y)\n",
    "            # Posterior is product of prior and likelihoods (ignoring scaling factor)\n",
    "            for feature_value, params in zip(sample, self.parameters[i]):\n",
    "                # Likelihood of feature value given distribution of feature values given y\n",
    "                likelihood = self._calculate_likelihood(params[\"mean\"], params[\"var\"], feature_value)\n",
    "                posterior *= likelihood\n",
    "            posteriors.append(posterior)\n",
    "        # Return the class with the largest posterior probability\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict the class labels of the samples in X \"\"\"\n",
    "        y_pred = [self._classify(sample) for sample in X]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.68      0.70      0.69       347\n",
      "        1.0       0.63      0.61      0.62       294\n",
      "\n",
      "avg / total       0.66      0.66      0.66       641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#knn_model=KNN(k=5)\n",
    "dataX=X.values\n",
    "dataY=Y.values\n",
    "testdataX=X_test.values\n",
    "testdataY=Y_test.values\n",
    "#print(dataX,dataY)\n",
    "clf = NaiveBayes()\n",
    "clf.fit(dataX,dataY)\n",
    "y_NBpred = clf.predict(testdataX)\n",
    "print(classification_report(Y_test,y_NBpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes from sklearn Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.72      0.58      0.64       347\n",
      "        1.0       0.60      0.74      0.66       294\n",
      "\n",
      "avg / total       0.67      0.65      0.65       641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_SKNBpred = gnb.fit(X, Y).predict(X_test)\n",
    "print(classification_report(Y_test,y_SKNBpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def standardize(X):\n",
    "    \"\"\" Standardize the dataset X \"\"\"\n",
    "    X_std = X\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    for col in range(np.shape(X)[1]):\n",
    "        if std[col]:\n",
    "            X_std[:, col] = (X_std[:, col] - mean[col]) / std[col]\n",
    "    # X_std = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    return X_std\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\" Returns the mean squared error between y_true and y_pred \"\"\"\n",
    "    mse = np.mean(np.power(y_true - y_pred, 2))\n",
    "    return mse\n",
    "def calculate_entropy(y):\n",
    "    \"\"\" Calculate the entropy of label array y \"\"\"\n",
    "    log2 = lambda x: math.log(x) / math.log(2)\n",
    "    unique_labels = np.unique(y)\n",
    "    entropy = 0\n",
    "    for label in unique_labels:\n",
    "        count = len(y[y == label])\n",
    "        p = count / len(y)\n",
    "        entropy += -p * log2(p)\n",
    "    return entropy\n",
    "def calculate_variance(X):\n",
    "    \"\"\" Return the variance of the features in dataset X \"\"\"\n",
    "    mean = np.ones(np.shape(X)) * X.mean(0)\n",
    "    n_samples = np.shape(X)[0]\n",
    "    variance = (1 / n_samples) * np.diag((X - mean).T.dot(X - mean))\n",
    "    \n",
    "    return variance\n",
    "\n",
    "def divide_on_feature(X, feature_i, threshold):\n",
    "    \"\"\" Divide dataset based on if sample value on feature index is larger than\n",
    "        the given threshold \"\"\"\n",
    "    split_func = None\n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "        split_func = lambda sample: sample[feature_i] >= threshold\n",
    "    else:\n",
    "        split_func = lambda sample: sample[feature_i] == threshold\n",
    "\n",
    "    X_1 = np.array([sample for sample in X if split_func(sample)])\n",
    "    X_2 = np.array([sample for sample in X if not split_func(sample)])\n",
    "\n",
    "    return np.array([X_1, X_2])\n",
    "\n",
    "\n",
    "class DecisionNode():\n",
    "    \"\"\"Class that represents a decision node or leaf in the decision tree\n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_i: int\n",
    "        Feature index which we want to use as the threshold measure.\n",
    "    threshold: float\n",
    "        The value that we will compare feature values at feature_i against to\n",
    "        determine the prediction.\n",
    "    value: float\n",
    "        The class prediction if classification tree, or float value if regression tree.\n",
    "    true_branch: DecisionNode\n",
    "        Next decision node for samples where features value met the threshold.\n",
    "    false_branch: DecisionNode\n",
    "        Next decision node for samples where features value did not meet the threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_i=None, threshold=None,\n",
    "                 value=None, true_branch=None, false_branch=None):\n",
    "        self.feature_i = feature_i          # Index for the feature that is tested\n",
    "        self.threshold = threshold          # Threshold value for feature\n",
    "        self.value = value                  # Value if the node is a leaf in the tree\n",
    "        self.true_branch = true_branch      # 'Left' subtree\n",
    "        self.false_branch = false_branch    # 'Right' subtree\n",
    "\n",
    "\n",
    "# Super class of RegressionTree and ClassificationTree\n",
    "class DecisionTree(object):\n",
    "    \"\"\"Super class of RegressionTree and ClassificationTree.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    min_samples_split: int\n",
    "        The minimum number of samples needed to make a split when building a tree.\n",
    "    min_impurity: float\n",
    "        The minimum impurity required to split the tree further.\n",
    "    max_depth: int\n",
    "        The maximum depth of a tree.\n",
    "    loss: function\n",
    "        Loss function that is used for Gradient Boosting models to calculate impurity.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_samples_split=2, min_impurity=1e-7,\n",
    "                 max_depth=float(\"inf\"), loss=None):\n",
    "        self.root = None  # Root node in dec. tree\n",
    "        # Minimum n of samples to justify split\n",
    "        self.min_samples_split = min_samples_split\n",
    "        # The minimum impurity to justify split\n",
    "        self.min_impurity = min_impurity\n",
    "        # The maximum depth to grow the tree to\n",
    "        self.max_depth = max_depth\n",
    "        # Function to calculate impurity (classif.=>info gain, regr=>variance reduct.)\n",
    "        self._impurity_calculation = None\n",
    "        # Function to determine prediction of y at leaf\n",
    "        self._leaf_value_calculation = None\n",
    "        # If y is one-hot encoded (multi-dim) or not (one-dim)\n",
    "        self.one_dim = None\n",
    "        # If Gradient Boost\n",
    "        self.loss = loss\n",
    "\n",
    "    def fit(self, X, y, loss=None):\n",
    "        \"\"\" Build decision tree \"\"\"\n",
    "        self.one_dim = len(np.shape(y)) == 1\n",
    "        self.root = self._build_tree(X, y)\n",
    "        self.loss=None\n",
    "\n",
    "    def _build_tree(self, X, y, current_depth=0):\n",
    "        \"\"\" Recursive method which builds out the decision tree and splits X and respective y\n",
    "        on the feature of X which (based on impurity) best separates the data\"\"\"\n",
    "\n",
    "        largest_impurity = 0\n",
    "        best_criteria = None    # Feature index and threshold\n",
    "        best_sets = None        # Subsets of the data\n",
    "\n",
    "        # Check if expansion of y is needed\n",
    "        if len(np.shape(y)) == 1:\n",
    "            y = np.expand_dims(y, axis=1)\n",
    "\n",
    "        # Add y as last column of X\n",
    "        Xy = np.concatenate((X, y), axis=1)\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
    "            # Calculate the impurity for each feature\n",
    "            for feature_i in range(n_features):\n",
    "                # All values of feature_i\n",
    "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
    "                unique_values = np.unique(feature_values)\n",
    "\n",
    "                # Iterate through all unique values of feature column i and\n",
    "                # calculate the impurity\n",
    "                for threshold in unique_values:\n",
    "                    # Divide X and y depending on if the feature value of X at index feature_i\n",
    "                    # meets the threshold\n",
    "                    Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)\n",
    "\n",
    "                    if len(Xy1) > 0 and len(Xy2) > 0:\n",
    "                        # Select the y-values of the two sets\n",
    "                        y1 = Xy1[:, n_features:]\n",
    "                        y2 = Xy2[:, n_features:]\n",
    "\n",
    "                        # Calculate impurity\n",
    "                        impurity = self._impurity_calculation(y, y1, y2)\n",
    "\n",
    "                        # If this threshold resulted in a higher information gain than previously\n",
    "                        # recorded save the threshold value and the feature\n",
    "                        # index\n",
    "                        if impurity > largest_impurity:\n",
    "                            largest_impurity = impurity\n",
    "                            best_criteria = {\"feature_i\": feature_i, \"threshold\": threshold}\n",
    "                            best_sets = {\n",
    "                                \"leftX\": Xy1[:, :n_features],   # X of left subtree\n",
    "                                \"lefty\": Xy1[:, n_features:],   # y of left subtree\n",
    "                                \"rightX\": Xy2[:, :n_features],  # X of right subtree\n",
    "                                \"righty\": Xy2[:, n_features:]   # y of right subtree\n",
    "                                }\n",
    "\n",
    "        if largest_impurity > self.min_impurity:\n",
    "            # Build subtrees for the right and left branches\n",
    "            true_branch = self._build_tree(best_sets[\"leftX\"], best_sets[\"lefty\"], current_depth + 1)\n",
    "            false_branch = self._build_tree(best_sets[\"rightX\"], best_sets[\"righty\"], current_depth + 1)\n",
    "            return DecisionNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\n",
    "                                \"threshold\"], true_branch=true_branch, false_branch=false_branch)\n",
    "\n",
    "        # We're at leaf => determine value\n",
    "        leaf_value = self._leaf_value_calculation(y)\n",
    "\n",
    "        return DecisionNode(value=leaf_value)\n",
    "\n",
    "\n",
    "    def predict_value(self, x, tree=None):\n",
    "        \"\"\" Do a recursive search down the tree and make a prediction of the data sample by the\n",
    "            value of the leaf that we end up at \"\"\"\n",
    "\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we have a value (i.e we're at a leaf) => return value as the prediction\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "\n",
    "        # Choose the feature that we will test\n",
    "        feature_value = x[tree.feature_i]\n",
    "\n",
    "        # Determine if we will follow left or right branch\n",
    "        branch = tree.false_branch\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            if feature_value >= tree.threshold:\n",
    "                branch = tree.true_branch\n",
    "        elif feature_value == tree.threshold:\n",
    "            branch = tree.true_branch\n",
    "\n",
    "        # Test subtree\n",
    "        return self.predict_value(x, branch)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Classify samples one by one and return the set of labels \"\"\"\n",
    "        y_pred = [self.predict_value(sample) for sample in X]\n",
    "        return y_pred\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        \"\"\" Recursively print the decision tree \"\"\"\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we're at leaf => print the label\n",
    "        if tree.value is not None:\n",
    "            print (tree.value)\n",
    "        # Go deeper down the tree\n",
    "        else:\n",
    "            # Print test\n",
    "            print (\"%s:%s? \" % (tree.feature_i, tree.threshold))\n",
    "            # Print the true scenario\n",
    "            print (\"%sT->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.true_branch, indent + indent)\n",
    "            # Print the false scenario\n",
    "            print (\"%sF->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.false_branch, indent + indent)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ClassificationTree(DecisionTree):\n",
    "    def _calculate_information_gain(self, y, y1, y2):\n",
    "        # Calculate information gain\n",
    "        p = len(y1) / len(y)\n",
    "        entropy = calculate_entropy(y)\n",
    "        info_gain = entropy - p * \\\n",
    "            calculate_entropy(y1) - (1 - p) * \\\n",
    "            calculate_entropy(y2)\n",
    "\n",
    "        return info_gain\n",
    "\n",
    "    def _majority_vote(self, y):\n",
    "        most_common = None\n",
    "        max_count = 0\n",
    "        for label in np.unique(y):\n",
    "            # Count number of occurences of samples with label\n",
    "            count = len(y[y == label])\n",
    "            if count > max_count:\n",
    "                most_common = label\n",
    "                max_count = count\n",
    "        return most_common\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._impurity_calculation = self._calculate_information_gain\n",
    "        self._leaf_value_calculation = self._majority_vote\n",
    "        super(ClassificationTree, self).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.72      0.76      0.74       347\n",
      "        1.0       0.70      0.65      0.67       294\n",
      "\n",
      "avg / total       0.71      0.71      0.71       641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataX=X.values\n",
    "dataY=Y.values\n",
    "testdataX=X_test.values\n",
    "testdataY=Y_test.values\n",
    "#print(dataX,dataY)\n",
    "clf = ClassificationTree()\n",
    "clf.fit(dataX, dataY)\n",
    "y_DTpred = clf.predict(testdataX)\n",
    "print(classification_report(Y_test,y_DTpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification tree from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.72      0.76      0.74       347\n",
      "        1.0       0.69      0.65      0.67       294\n",
      "\n",
      "avg / total       0.71      0.71      0.71       641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "y_DTSKpred=clf.predict(X_test)\n",
    "print(classification_report(Y_test,y_DTSKpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada Boost from sklearn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bunny\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.76      0.88      0.81       347\n",
      "        1.0       0.82      0.67      0.74       294\n",
      "\n",
      "avg / total       0.79      0.78      0.78       641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# Create adaboost classifer object\n",
    "abc = AdaBoostClassifier(n_estimators=30,\n",
    "                         learning_rate=0.4)\n",
    "# Train Adaboost Classifer\n",
    "model = abc.fit(X, Y)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_adaSKpred = model.predict(X_test)\n",
    "print(classification_report(Y_test,y_adaSKpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost from package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.76      0.87      0.81       347\n",
      "        1.0       0.81      0.68      0.74       294\n",
      "\n",
      "avg / total       0.79      0.78      0.78       641\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bunny\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier(n_estimators=50, max_depth=2, learning_rate=0.2, subsample=0.5)\n",
    "dataX=X.values\n",
    "dataY=Y.values\n",
    "testdataX=X_test.values\n",
    "testdataY=Y_test.values\n",
    "train_model = model.fit(dataX, dataY)\n",
    "xgpred = train_model.predict(testdataX)\n",
    "print(classification_report(Y_test,xgpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM from package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.75      0.88      0.81       347\n",
      "        1.0       0.82      0.65      0.73       294\n",
      "\n",
      "avg / total       0.78      0.78      0.77       641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "dataX=X.values\n",
    "dataY=Y.values\n",
    "testdataX=X_test.values\n",
    "testdataY=Y_test.values\n",
    "svclassifier = SVC(kernel='linear')  \n",
    "svclassifier.fit(dataX, dataY)\n",
    "y_pred = svclassifier.predict(testdataX)\n",
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bunny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "dataX=X.values\n",
    "dataY=Y.values\n",
    "testdataX=X_test.values\n",
    "testdataY=Y_test.values\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=89, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bunny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "3048/3048 [==============================] - 3s 880us/step - loss: 0.6103 - acc: 0.6673\n",
      "Epoch 2/20\n",
      "3048/3048 [==============================] - 1s 195us/step - loss: 0.4748 - acc: 0.7730\n",
      "Epoch 3/20\n",
      "3048/3048 [==============================] - 1s 198us/step - loss: 0.4572 - acc: 0.7792\n",
      "Epoch 4/20\n",
      "3048/3048 [==============================] - 1s 192us/step - loss: 0.4476 - acc: 0.7890\n",
      "Epoch 5/20\n",
      "3048/3048 [==============================] - 1s 196us/step - loss: 0.4435 - acc: 0.7913\n",
      "Epoch 6/20\n",
      "3048/3048 [==============================] - 1s 243us/step - loss: 0.4434 - acc: 0.7943\n",
      "Epoch 7/20\n",
      "3048/3048 [==============================] - 1s 222us/step - loss: 0.4389 - acc: 0.7940\n",
      "Epoch 8/20\n",
      "3048/3048 [==============================] - 1s 201us/step - loss: 0.4412 - acc: 0.8025\n",
      "Epoch 9/20\n",
      "3048/3048 [==============================] - 1s 198us/step - loss: 0.4385 - acc: 0.7956\n",
      "Epoch 10/20\n",
      "3048/3048 [==============================] - 1s 198us/step - loss: 0.4395 - acc: 0.8015\n",
      "Epoch 11/20\n",
      "3048/3048 [==============================] - 1s 198us/step - loss: 0.4403 - acc: 0.7894\n",
      "Epoch 12/20\n",
      "3048/3048 [==============================] - 1s 193us/step - loss: 0.4320 - acc: 0.8009\n",
      "Epoch 13/20\n",
      "3048/3048 [==============================] - 1s 200us/step - loss: 0.4307 - acc: 0.7999\n",
      "Epoch 14/20\n",
      "3048/3048 [==============================] - 1s 194us/step - loss: 0.4301 - acc: 0.8048\n",
      "Epoch 15/20\n",
      "3048/3048 [==============================] - 1s 195us/step - loss: 0.4329 - acc: 0.7956\n",
      "Epoch 16/20\n",
      "3048/3048 [==============================] - 1s 200us/step - loss: 0.4307 - acc: 0.7986\n",
      "Epoch 17/20\n",
      "3048/3048 [==============================] - 1s 195us/step - loss: 0.4269 - acc: 0.8012\n",
      "Epoch 18/20\n",
      "3048/3048 [==============================] - 1s 176us/step - loss: 0.4261 - acc: 0.8031\n",
      "Epoch 19/20\n",
      "3048/3048 [==============================] - 1s 194us/step - loss: 0.4265 - acc: 0.8031\n",
      "Epoch 20/20\n",
      "3048/3048 [==============================] - 1s 191us/step - loss: 0.4239 - acc: 0.8009\n",
      "641/641 [==============================] - 0s 421us/step\n",
      "\n",
      "acc: 77.54%\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataX, dataY, epochs=20, batch_size=10)\n",
    "scores = model.evaluate(testdataX,Y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
